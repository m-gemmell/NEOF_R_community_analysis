--- 
title: "R community analysis"
author: "Matthew R. Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
favicon: figures/NEOF_favicon.png
description: NEOF book for the R community analysis workflow
cover-image: "figures/NEOF.png"
---
```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```

```{r cite-packages, include = FALSE}
# automatically create a bib database for R packages
# add any packages you want to cite here
knitr::write_bib(c(
  .packages(), 'bookdown', 'webexercises'
), 'packages.bib')
```

```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/NEOF.png", auto_pdf = TRUE)
``` 

# (PART\*) Intro {-}

# Introduction
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/R_community.png", auto_pdf = TRUE)
``` 

A lot of different analysis and visualisations can be carried out with community data. This includes taxonomy and functional abundance tables from 16S rRNA and Shotgun metagenomics analysis. This workshop will teach you how to use R with the phyloseq R object; a specialised object containing an abundance, taxonomy, and metadata table. 

The workshop will use a 16S dataset that has been pre-analysed with QIIME2 to create the ASV table, taxonomy table, and phylogenetic tree. Supplementary materials will show how to import Bracken shotgun metagenomic abundance data and generic abundance data frames into a phyloseq object.
 
Sessions will start with a brief presentation followed by self-paced computer practicals guided by an online interactive book. The book will contain theory and practice code. This will be reinforced with multiple choice questions that will recap concepts and aid in interpretation of results.

At the end of the course learners will be able to:

- Import QIIME2 artifacts into a phyloseq object.
- Summarise the abundance and taxonomy contents of a phyloseq object
- Preprocess the abundance and taxonomy tables. This will include transforming sample counts, and subsetting samples & taxonomies.
- Understand the grammar of graphics (ggplot2) used by phyloseq and related packages.
- Carry out alpha & beta diversity, and biomarker detection with the phyloseq object.
- Produce and customise publication quality plots.
- Run statistical analysis and incorporate these values into the plots.
- Convert static plots into interactive html plots with plotly within R.



<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

<!--chapter:end:01-R_community_main_workflow.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Dataset & workflow
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/data.png", auto_pdf = TRUE)
``` 

## Dataset
```{r, fig.align = 'center',out.width= '50%', echo=FALSE }
knitr::include_graphics(path = "figures/freshwater_france.png", auto_pdf = TRUE)
``` 

In this tutorial we will be using 16S metabarcdoing datasets derived from surface water from the Durance River in the south-east of France. Two major comparisons were carried out in combination with each other.

### Sites
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/river.png", auto_pdf = TRUE)
``` 

Three different sites were chosen on the Durance River. These three sites were representative of an anthropisation (transformation of land by humans) gradient along a river stream. These sites were:

- __Upper Durance sampling site (UD)__: Bottom part of the alpine part of the river with little/no anthropisation.
- __Middle Durance sampling site (MD)__: Upper part of agricultural land dominated by apple and pear production.
- __Lower Durance sampling site (LD)__: Lower part of agricultural land with intensive production of fruits, cereals, and vegetables.

### Culture media
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/petri.png", auto_pdf = TRUE)
```

Surface water was sampled and different culture media were used to produce bacterial lawns for each site. The media used were:

- __Environmental sample (ENV)__: No media used, frozen at -20°C will DNA extraction.
- __TSA 10%__ incubated at 28°C for 2 days.
- __KBC__ incubated at 28°C for 2 days.
- __CVP__ incubated at 28°C for 3 days.

### Summary & questions
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/sum_red.png", auto_pdf = TRUE)
```

Each sample and media combination was produced in replicates of three giving a total of 27 samples (3*4*3 = 36). The three replicates were cultured on three different plates with the same media. An ASV table, taxonomy table, and phylogenetic tree were produced with QIIME2 and DADA2.

With this data we can ask and investigate the following questions:

- How does the bacterial communities change across the anthropisation gradient?
- Is there a difference in the replicates of one site and media combination. I.e. do any of the media produce inconsistent profiles.
- Is there more difference between the sites or the media used?
- Do any of the media produce a similar taxonomic profile to the environmental sample?


## Workflow
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/workflow.png", auto_pdf = TRUE)
```


<!--chapter:end:02-Dataset_and_workflow.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# R Packages
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/R.png", auto_pdf = TRUE)
```  

During this workshop we will use various R packages with their own intricacies. Before going into analysis we'll introduce you to some of these important concepts.

## R packages/libraries
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/r_package.png", auto_pdf = TRUE)
```

R packages/libraries contain additional functions, data and code for analysing, manipulating and plotting different types of data. Many common packages will be installed as default when you install R. Other more specialised packages, such as the `ggplot2` package, must be installed by the user.

Packages found on The Comprehensive R Archive Network (CRAN) which is R’s central software repository can be installed easily using the following command.

```{r eval=FALSE}
install.packages("package_name")
```

Every time you reload R you will need to load the packages you need if they are not one of the ones installed by default. To do this type:

```{r eval=FALSE}
library("package_name")
```

I generally have a list of `library()` functions at the top of my R scripts (`.R` files) for all the packages I use in the script.

Throughout this course you will get a lot of practice installing and loading various packages.

`r hide("R package or R Library?")`
R packages are a collection of R functions, data, and compiled code. You can install these into a directory on your computer.

An R library is a directory containing a R package.

Because of this, the terms R package and R library may be used synonymously. We will use the term package in this workshop.
`r unhide()`

As we will be using a lot of packages we shall use a double colons to specify which package each function belongs to, unless the function is from base R. For example if we use the function `summarize_phyloseq()` from the package `microbiome` we would type the function like below:

__Note__: Do not run the below command.

```{r eval=FALSE}
microbiome::summarize_phyloseq()
```

This convention has 2 benefits:

- We can easily tell which R package each function comes from.
  - This is useful for your future coding where you may copy some, but not all, commands from one script to another. You will therefore know which packages you will need to load.
  - If you need some more documentation about a function you will know what package to look up.
  - Writing your methods will be a lot easier.
- Different packages may have functions with the same name. Specifying the package will ensure you are using the correct function.

## The grammar of graphics
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/ggplot2.png", auto_pdf = TRUE)
``` 

During this course we will be using the grammar of graphics coding approach. This approach is implemented by the R package `ggplot2` to create visualisations such as bar charts, box plots, ordination plots etc. In turn `ggplot2` is used by a host of other packages, some of which we will be using. Although `ggplot2` is R code its structure is very different and it takes effort to learn. Thankfully, `ggplot2` is very powerful and flexible, and it produces very professional and clean plots.

We will use the `iris` dataset (inbuilt into R) to show an example of `ggplot2` code and its visualisation output is:

__Note__: If you would like to see the contents of the `iris` dataset you can run the command `View(iris)` in your R instance.

```{r}
#Load library
library(ggplot2)

#Create new ggplot2 object using iris dataset
ggplot2::ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, colour=Species)) +
  #Make the object a scatter plot 
  ggplot2::geom_point() +
  #Add plot tile
  ggplot2::ggtitle("Iris Sepal length vs width") +
  #Set x and y axis label names
  ggplot2::labs(x = "Sepal length", y = "Sepal width")
```

We will not learn `ggplot2` specifically during this course. However, the structure of creating an object will be used. In the above case the initial object was built with `ggplot`. Subsequently additions and edits were carried out with `+` and various other functions.

An important concept of the grammar of graphics is aesthetics. Aesthetics are the parts of a graphic/plot. In the above command we set the aesthetics with the function `aes()` within the `ggplot()` function. The X aesthetic (i.e. what values are assigned to the x axis) was set as the Sepal length values from the column `Sepal.Length` of the dataframe `iris`. In turn the Y axis values are set to the Sepal width and the colouring of the points are set to the Species.

That was a quick introduction to the grammar of graphics. We will be using this to create visualisations with a `phyloseq` object using various R packages specifically designed for community abundance data within `phyloseq` objects.

For more resources on `ggplot2` please see the [appendix](#ggplot2_appendix) of this book.

## phyloseq
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/phyloseq_logo.png", auto_pdf = TRUE)
``` 

In this book we will be working with `phyloseq` objects to preprocess  our dataset, create visualisations, and carry out statistical analyses. This is a very popular object type for community abundance datasets as it contains the abundance table, metadata, and taxonomy table in one object, optionally containing the phylogenetic tree and reference sequences if wanted/required.

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/phyloseq_input.png", auto_pdf = TRUE)
``` 

<!--chapter:end:03-R_packages.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Set-up
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/start.png", auto_pdf = TRUE)
```

Prior to any analysis we need to setup our environment in the webVNC.

## Logon instructions {#cluster}

For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent.

You will now be in a logged-in Linux VNC desktop with two terminals. 
You will see something as below (there may be only one terminal which is fine). 
If you do not see something similar please ask for assistance.

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/nsc200_logon.png", auto_pdf = TRUE)
```

If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. 
You will most likely need to use your browser's tool bar to accomplish this. 
Ensure you can see the grey borders.

These instructions will not work outside of this workshop. 
If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux 

The following link is a guide to install Mint Linux:  
https://linuxmint-installation-guide.readthedocs.io/en/latest/

## Mamba
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/mamba_logo.png", auto_pdf = TRUE)
```

This workshop requires a lot of packages. 
These all can be difficult to install with R. 
Instead we have used Mamba forge to install R, its packages, and Jupyter-notebook (more info below). 
To learn more about Mamba-forge and how to create your own environment please see the [appendix](#https://github.com/mamba-org/mamba).

To set-up your environment for this workshop please run the following code (you must include the full stop and space at the front of the command).

```{bash, eval=FALSE}
. usercommunity
```

You will have successfully activated the environment if you now see `(r_community)` at the start of your command prompt. 
This indicates you are now in the mamba environment called `r_community` created by the instructor.

If you are interested in the use script you can look at its contents.

```{bash, eval=FALSE}
less /usr/local/bin/usercommunity
```

__Tip:__ press `q` to quit `less`.

<!--chapter:end:04-Setup.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Jupyter
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_logo.png", auto_pdf = TRUE)
```

[`Jupyter-notebook`](https://jupyter.org/) is a nice browser based method to write, edit, and run code.
It was initally created for Python coding, but has since branched out to many other languages, such as `R`.

We are using it in this workshop for a variety of its properties:

- It is popular and well maintained.
- It is lightweight. Other heavier weight programs, such as RStudio, would struggle in our HPC due to the graphical and CPU load.
- It is interactive and displays code output. 
- It allows for easier annotation, editing, and debugging than the command line. 
- It provides a graphical interface for hanging directories and choosing files.

Before carrying out any analysis we will go through a quick tutorial of `jupyter-notebook`.

## Open Jupyter-notebook

The first step is to open `jupyter-notebook`.
Run the below command in your `(r_community)` environment.

```{bash, eval=FALSE}
jupyter-notebook
```

This will open `jupyter-notebook` in firefox. We won't need to access the linux terminal anymore. Leave it running `jupyter-notebook` and full screen your firefox so you should see something like below.

```{r, fig.align = 'center',out.width= '100%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_notebook_example_1.png", auto_pdf = TRUE)
```

## Create R notebook
The next step is to create a R notebook.

1. Click on the __"New"__ button towards the top right, right of the "Upload" button.
2. From the dropdown click __"R"__ below "Python 3 (ipykernel)".

This will open up a new R notebook like below.

```{r, fig.align = 'center',out.width= '100%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_notebook_example_2.png", auto_pdf = TRUE)
```

## Cells and code

Jupyter-notebook uses cells (the gray boxes) to separate code. 
This is very useful to compartmentalise our code.

There will already be one cell. Within the cell, type in the below commands.

```{r, eval=FALSE}
1+1
2-3
```

When pressing enter in cells it will create a new line.
To run all commands in a cell press `CTRL + enter`. 

Run your current cell and you should see something like below.

```{r, fig.align = 'center',out.width= '100%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_notebook_example_3.png", auto_pdf = TRUE)
```

## Create new cells

You can create new cells by 2 different means.

- Press the `+` button on the tool bar (between the floppy disk `r icons::ionicons("save-outline")` and scissors `r icons::ionicons("cut")`). This will add a cell below your currently selected option.
- Click on the __`Insert`__ button and use the dropdown to add a cell above or below your currently selected cell.

__Tip:__ Hover over the toolbar icons to display a text based description of its function.

With that knowledge add a second cell below the first cell. 
Add the following code to your second cell but do not run it.

```{r, eval=FALSE}
num_1 <- 3
num_2 <- 10
```

__Tip:__ Notice there are green lines around your selected cell.

Insert a third cell and add the following code to it. Do not run the code.

```{r, eval=FALSE}
num_1 * num_2
```

## Running code

Try to run the code in the third cell. 
There should be an error as we have not created the objects `num_1` & `num_2`.
We have only written the code for these objects but not run them.

We can run all the code in a notebook starting from the first cell to the last cell.

Two methods to run all cells are:

- Click on the __"Cell"__ button.
- Click __"Run All"__ from the drop-down options.

You should then see something like the below in your notebook.

```{r, fig.align = 'center',out.width= '100%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_notebook_example_4.png", auto_pdf = TRUE)
```

There is no output printed for cell 2 because we are assigning variables. However, now there is the correct output for Cell 3 as the variables were assigned before the command was run.

## Saving the file

As with RStudio and other good coding interfaces we can save our notebook.

First we should rename the file. Rename the notebook to __"jupyter_tut"__:

1. Click on the name of the notebook, currently called __"Untitled"__.
  - This is at the very top of the notebook, right of the Jupyter logo.
2. A pop-up called __"Rename Notebook"__ will appear. Change the Name to __"jupyter_tut"__.
3. Click __"Rename"__.

Now we can save the file. Two methods to save are:

- Click the floppy disk `r icons::ionicons("save-outline")` on the toolbar.
- Click on the __"File"__ button. Click __"Save and Checkpoint"__ from the dropdown options.

## Title cells with markdown

We will be using multiple notebooks in this workshop. 
However, we will also have multiple sections per notebook. 
It will therefore be useful to create header cells with markdown to create visual separation of the different sections.

To add a header cell to the top of our notebook:

- Create a new cell at the top of the notebook.
- Click on the __"Code"__ drop down and select __"Markdown"__.
  - The __"Heading"__ option no longer works.

```{r, fig.align = 'center',out.width= '100%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_notebook_example_5.png", auto_pdf = TRUE)
```

- Add the following to the __"Markdown"__ cell to create a first level header.
  - Ensure you have a space between the `#` and header text ("Tutorial").
```{r, eval=FALSE}
# Tutorial
```

Great, we can now add nice headers in our notebooks. __Save__ the notebook once more before carrying on to the next section.

`r hide("Markdown")`
You won't need to know more about `Markdown` but if you are interested please see the [`Markdown` guide](https://www.markdownguide.org/basic-syntax/).
`r unhide()`

## Close the notebook

To close the notebook:

- Click on __"File"__.
- From the dropdown options click __"Close and Halt"__.

When you are back in the file explorer page you may not yet set the new file you saved.
If so, you will need to refresh the page with the Refresh button `r icons::ionicons("refresh")` towards the top right.

```{r, fig.align = 'center',out.width= '100%', echo=FALSE }
knitr::include_graphics(path = "figures/jupyter_notebook_refresh.png", auto_pdf = TRUE)
```

With that quick tutorial of `jupyter-notebook` we can start our community analysis ion the next chapter.

## Video tutorial


<div class="container">
<iframe src="https://www.youtube.com/embed/-c_6HoPMw9g" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>


<!--chapter:end:05-Jupyter.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# (PART\*) Data preparation {-}

# Data prep intro
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/half_tablespoon.png", auto_pdf = TRUE)
```

In the next 5 chapters (7-11) we will learn how to:

- [Import our data.](#import_chap)
- [Summarise our `phyloseq` object.](#sum_phyloseq_chap)
- Transform our abundance `phyloseq` object into:
  - A relative/compositional abundance `phyloseq` object
  - A rarefied `phyloseq` object.

We will also use this as a chance to reinforce how to use `jupyter-notebook` with clear instructions. 
However, from chapters 10 onwards you will make more decisions on how many __"Coding"__ and __"Markdown"__ cells you want.

<!--chapter:end:06-Preprocess_part.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Import {#import_chap}
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/import.png", auto_pdf = TRUE)
```

Before carrying out any analysis we first need to import our QIIME2 artifacts into a phyloseq object. Thankfully there is an R package called `qiime2R`

## Import: notebook

```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/r_script.png", auto_pdf = TRUE)
```

Prior to any coding, we will create a new directory, our analysis directory, for this workshop and create a new notebook called __"01-Import.ipynb"__ in it. We will be creating a new notebook for each chapter and numbering them so we can easily see the order of scripts.

First create a new directory.

- In the notebook file explorer, click the __"New"__ button.
- Select __"Folder"__

You will have an __"Untitled Folder"__. To rename it:

- Click on the box left of the name.
- Press the __"Rename"__ button that appeared.
- Change the name to __"R_community_workshop"__.
- Click __"Rename"__.

Click on your __"R_community_workshop"__ folder to move into it.

Next step is to create a new R notebook, rename it to __"01-Import"__, and save it.

## qiime2R

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/qiime2r.png", auto_pdf = TRUE)
```

[`qiime2R`](https://github.com/jbisanz/qiime2R) is an R package for importing QIIME2 artifacts into a R phyloseq object. The package contains many different commands. Its function `read_qza()` can read a single artifact at a time.

The best way to import all your QIIME2 artifacts is with the `qza_to_phyloseq()` function. In your "01-Import.R" script, add the following and run the commands.

__Tip__: You can tab complete and/or copy and paste file paths within the webVNC.

```{r, eval=FALSE}
#Cell 1
#Load the package/library
library("qiime2R")

#Import data
pseq <- qiime2R::qza_to_phyloseq(
  features = "/pub14/tea/nsc206/NEOF/R_community/data/table-dada2.qza",
  tree = "/pub14/tea/nsc206/NEOF/R_community/data/rooted-tree.qza",
  taxonomy = "/pub14/tea/nsc206/NEOF/R_community/data/taxonomy.sklearn.qza",
  metadata = "/pub14/tea/nsc206/NEOF/R_community/data/media_metadata.txt"
)
```

This command creates a phyloseq object named `pseq`. It contains:

-   The ASV abundance table (`features = "table-dada2.qza"`).
-   The rooted phylogenetic tree (`tree = "rooted-tree.qza"`).
-   The taxonomic classifications of the ASVs (`taxonomy = "taxonomy.sklearn.qza"`).
-   The sample metadata (`metadata = "media_metadata.txt"`)

## Import: Summarise phyloseq
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/list_blue.png", auto_pdf = TRUE)
```

Now that we have imported the data we can extract some summary information from it.

First we will use the `microbiome` package with its `summarize_phyloseq()` function.

Create a new cell and write and run the below in it.

```{r, eval=FALSE}
#Cell 2
#Load microbiome library
library("microbiome")
#Summary of phyloseq object
microbiome::summarize_phyloseq(pseq)
```

This gives us a plethora of information:

-   The top line tells us if the data is compositional (relative abundance).
-   We get the following list of values in a paragraph and via a list.
    -   Min. number of reads: Number of reads in the sample with the lowest number of reads.
    -   Max. number of reads: Number of reads in the sample with the largest number of reads.
    -   Total number of reads: Sum of all reads across all samples.
    -   Average number of reads: Sum of all reads / number of samples.
    -   Median number of reads: Midpoint read abundance across samples.
    -   Sparsity: See expandable box further down.
    -   Any OTU sum to 1 or less?: States if there are any ASVs with a summed abundance of 1 or less across all the samples. 
    -   Number of singletons: Number of ASVs with a sum of 1 or less across all samples.
    -   Percent of OTUs that are singletons: Percentage of ASVs that only contain one read across all the samples.
    -   Number of sample variables are: Number of sample variables/groupings in our metadata.
    -   The last line shows the names of the sample variables/groupings in our metadata.

`r hide("Sparsity")`
Sparsity is a measure of the number of 0s in a table. It can be represented by the following equation:

$$
sparsity = Z/C
$$

Where:

-   Z = The number of cells that equal zero.
-   C = The total number of cells.

Let's look at an example of an abundance table with a small amount of ASVs and Samples.

|      | Sample1 | Sample2 | Sample3 |
|------|---------|---------|---------|
| ASV1 | 0       | 10      | 24      |
| ASV2 | 1       | 0       | 37      |
| ASV3 | 6       | 25      | 0       |
| ASV4 | 51      | 2       | 0       |

- This abundance table has 12 cells, 3 samples \* 4 ASVs. 
- Of these cells, 4 have an abundance of zero. 
- 4/12 = 0.3333, therefore its sparsity is 0.3333.

Sparsity can be any value from 0-1. The higher the value the more sparse it is, with a value of 1 meaning all the cells have an abundance of zero. The lower the value the less sparse it is, with a value of 0 meaning all the cells have an abundance of 1 or more.

16S data is known to be sparse so high sparsity is not unexpected. Keep in mind that lower levels of taxa (ASVs, Species, & Genera) will generally have more sparse tables that higher levels of taxa (Kingdom, Phylum, Class).
`r unhide()`

If you would like to see how the function calculates its values you can view the [source code online](https://rdrr.io/github/microbiome/microbiome/src/R/summarize_phyloseq.R).

## Save the phyloseq object
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/r_save.png", auto_pdf = TRUE)
```

When using multiple notebooks/scripts for analysis it is useful to save the R objects that will be used in different notebooks/scripts. This can be carried out with the function `save()`.

Write and run the following code in a third cell.

```{r, eval=FALSE}
#Cell 3
#Save phyloseq as file
save(pseq, file = "phyloseq.RData")
```

This saves our object `pseq` into the file `phyloseq.RData`. The suffix `.RData` is the normal convention.

We have saved our final object of the notebook. Close and halt it.

## Import: recap
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/recap.png", auto_pdf = TRUE)
```

We have imported our QIIME2 artifacts into one phyloseq object so we can analyse the data in R. This object has been saved into a ".RData" file which we will load in the next chapter.

<!--chapter:end:07-Import.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Summarise phyloseq {#sum_phyloseq_chap}
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/transform.png", auto_pdf = TRUE)
```

For the next X chapters we will use one notebook. We will setup this notebook with libraries and `phyloseq` object we created in the last chapter.

After this, we will summarise the `phyloseq` object. We will investigate the read depth of samples and the number of ASVs in our dataset.

## Setup
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/setup_2.png", auto_pdf = TRUE)
```

Before starting analysis create and save a new `R` notebook called __"02-Preprocess.ipynb"__ in the analysis directory (when renaming you don't need to include the suffix).
We will use this notebook for this and the next chapter.

It is useful to add a title to the top of the notebook. 
Create a __"Markdown"__ cell and add the following first level header:

```{r, eval=FALSE}
#Preprocessing data notebook
```

The first section of our notebook will be used for setup.
This will involve loading libraries and data we need. 

For good documentation add the below second level heading to the first __"Markdown"__ cell. 

```{r, eval=FALSE}
## Setup
```

To decrease the level of a heading add another `#`.

- `# `: 1st level header.
- `## `: 2nd level header.
- `### `: 3rd level header etc...

I like to load all the libraries to be used in the notebook in this section. We will explain their uses later in this chapter.

Add the below to a new __"Code"__ cell in your notebook:

```{r, eval=FALSE}
#Libraries
library("phyloseq")
library("microbiome")
library("vegan")
```

Our last bit of set-up is to load in our abundance phyloseq object we created in the previous chapter.

```{r, eval=FALSE}
#Load the phyloseq object
load("phyloseq.RData")
```

Ensure you have run the code in this cell.

### Summarise header
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/header.png", auto_pdf = TRUE)
```

When you load in a dataset it is always useful to check it. 
We will therefore use a new section to inspect the data.

In a new __"Markdown"__ cell add the following 2nd level header.

```{r, eval=FALSE}
# Summarise the phyloseq object
```

### Summarise phyloseq
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/list_blue.png", auto_pdf = TRUE)
```

Create a second cell and add the following annotation and code. Then run the code.

```{r, eval=FALSE}
#Summary of phyloseq object
microbiome::summarize_phyloseq(pseq)
```

We ran this code in the __"01-Import.ipynb"__. You can therefore check if our new output matched the output from that notebook. This should be the case since they are the same data.

Due to the relative large amount of output to screen we'll put the next part into a new cell (third cell).

### Reads per sample
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/histogram.png", auto_pdf = TRUE)
```

In our third cell we will use the command `microbiome::readcount()` to store and display the number of reads in each sample within our `phyloseq` object (`pseq`). We can even make a quick histogram of read numbers per sample with the base `R` function `hist()`.

Write and run the below code in a third cell.

```{r, eval=FALSE}
#Number of reads per sample
reads_sample <- microbiome::readcount(pseq)
reads_sample
#Histogram
hist(reads_samples, "Histogram of read depths")
```

This information is very useful. We will use it in the next chapter to determine what our minimum read depth should be.

### ASVs per sample
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/nice_table.png", auto_pdf = TRUE)
```

The last feature we will look at before some preprocessing is the ASVs (Amplicon Feature Variants).

A useful `phyloseq` command is `otu_table()`. This allows us to extract the ASV/OTU/feature table. With this we can then carry out some other commands like looking at the total number of ASVs.

For demonstrative purposes write and run the following code in its own cell to display the ASV table.

```{r, eval=FALSE}
#Can extract ASV table (known as otu table in phyloseq)
phyloseq::otu_table(pseq)
```

With this command we will extract the number of ASVs in the original abundance table. 
We will then save this in a vector.
We will add to this vector as we create our relative abundance and rarefied abundance `phyloseq` objects. 

```{r, eval=FALSE}
#Each row is an ASV and each column is the samples
#Therefore we can get the number of ASVs in data
#Let's make a new vector with this info so we can easily keep track
num_asvs_vec <- c(nrow(phyloseq::otu_table(pseq)))
#Give the 1st element a relevant name
names(num_asvs_vec)[1] <- "abundance"
#View current vector
num_asvs_vec
```

Save this vector as a `.Rdata` object in a new cell.
This will allow us to load it in future notebooks.
Additionally, we can remove the object so it doesn't use RAM.

```{r, eval=FALSE}
#Save object as file
save(num_asvs_vec, file= "num_asvs_vec.RData")
#Remove object from environment
rm(num_asvs_vec)
```

## Summarise: recap
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/recap.png", auto_pdf = TRUE)
```

We now have an idea of some of the attributes of our dataset. We can use this knowledge to carry out some preprocessing.

<!--chapter:end:08-Summarise_phyloseq.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Minimum read depth
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/depth_diver.png", auto_pdf = TRUE)
``` 

It is good to remove samples with a very low read depth (number of sequencing reads).
However, it is not trivial to determine what an appropriate read depth is.
This value will vary from study to study.
Normally, for 16S data, a depth of at least 20K per sample is suggested. However, this is the general consensus for human microbiome data. 

In this chapter we will:

- Cover a brief intro to considertions of what is an acceptable minimum depth for your dataset.
- Reinvestigate our sample depths with a previously created histogram.
- View the depth ranges of different sample groups (site & media) with box plots.
- Create a rarefaction curve to assess if the depth of our samples have captured a good amount of biodiversity.
- Demonstrate how to filter samples by depth.

## Considerations
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/consider.png", auto_pdf = TRUE)
``` 

There are 3 main considerations to take into account for what is an appropriate depth for your dataset.

- __The biodiversity of your samples.__
  - If your sample is very biodiverse, such as the human gut microbiome, you will need a good depth (>20K per sample).
  - If your sample is less biodiverse, such as many geological environments or skin, then you will not need as much read depth.
  - Rarefaction curves are a good method to determine if your samples have enough depth. We will look at this in this chapter.
- __The biomass of your samples.__
  - Some environments are hard to extract DNA from.
  - If this is the case for you, then people will hopefully accept that this is an unfortunate reality of life and you will use what you can.
  - However, be careful of your conclusions, if you think your data doesn't have as much as it could do not make very definitive detailed claims.
- __Read depth of sample groups.__
  - It may be possible that a few samples have a much lower depth than the rest and so you may think to remove them.
  - However, these may all come from the same sample group and so you have lost all information of one group.
  - For instance, you may be comparing different geological surfaces and your rock samples have much lower read depths than the various soil samples.
  - For comparisons including the lower depth sample group (e.g. rock samples) you will need to retrain the lower depth sample.

That is a brief overview of that topic. If you are interested in more I suggest you look at papers where they have studied an environment similar to yours.

## Minimum read depth section title
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/header_l2.png", auto_pdf = TRUE)
``` 

In the next section of our `jupyter-notebook` we will investigate what the minimum read depth should be and remove sample below this.

Create a new __"Markdown"__ cell and add the following 2nd level header.

```{r, eval=FALSE}
## Minimum read depth
```

## Read depth vector and histogram
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/histogram.png", auto_pdf = TRUE)
```

We have already created a vector and histogram of the read depths across our samples. Scroll up your notebook to view these and answer the following MCQ:

```{r, echo = FALSE}
opts_p <- c("__1-8__", answer="__10,000-18,000__", "__20,000+__")
```
- What is the approximate read depth range of our dataset? `r longmcq(opts_p)`

This is a good first step but what if we want to know how the read depths vary between sample groups?

## Sample depth boxplot
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/boxplot.png", auto_pdf = TRUE)
```

We are going to use `ggplot2` to create a couple boxplots to show the sequencing depth ranges of the different sample groups (Site & Media).

### Creating data frame for boxplots
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/metadata.png", auto_pdf = TRUE)
```

First, we need to create an object containing our sample names, Site & Media information, and the depth.
We will use this object to produce our boxplots.
Thankfully the metadata in our `phyloseq` object contains all this information except the depth.

In a new cell write and run the below code. This will extract the sample data (metadata) to a new object and display the top 6 rows fo this new object.

```{r, eval=FALSE}
#Extract sample data as a separate R object
abundance_metadf <- phyloseq::sample_data(pseq)
#View top 6 rows of metadata data frame
head(abundance_metadf)
```

The 2 new functions above are:

- `phyloseq::sample_data()`: Extracts the sample data (metadata) data frame from a `phyloseq` object.
- `head()`: Returns the first 6 parts of an R object by default.
  - It can be used for a vector, matrix, table, data frame, or function.
  - In the case of a data frame it returns the first 6 rows.

We need to add the depth information to our new data frame. 
We have extracted this previously into an object called `sample_depths`. 
However, before adding it we want to check it has the same order of samples as the rows in `abundance_metadf`.

Write and run the below script in the same cell. The code uses `head()` to view the first 6 elements of `sample_depths` and the row names of `abundance_metadf`. Then the function `idnetical()` is used to see if they are identical (`TRUE`) or not (`FALSE`).

```{r, eval=FALSE}
#Check if our vector of sample_depths has the same order as our metadata rows
head(names(sample_depths))
head(row.names(abundance_metadf))
identical(names(sample_depths),row.names(abundance_metadf))
```

The order of samples is identical so we can add the depth information to `abundance_metadf`. Carry this out in the same cell with the code below.

```{r, eval=FALSE}
#Add sample depths to metadata data frame
abundance_metadf[,"depth"] <- sample_depths
#View top 6 rows of edited metadata dataframe
head(abundance_metadf)
```

Great! We will next use this data frame to create 2 boxplots.

### Depth boxplots
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/boxplots_groups.png", auto_pdf = TRUE)
```

We are going to create 2 boxplots with `ggplot2`.
We won't go into too much detail on how the code works here, instead learning more later in this book.

The code below creates a `ggplot2` boxplot. We carry this out with 2 functions:

- `ggplot()`: This creates a `ggplot2` object, storing the information and aesthetics.
  - The first option is the data we want to use for plotting (`abundance_metadf`).
  - The second option is the aesthetics (`aes()`) to plot. In this case we want the depth column to be plotted on the y-axis (`y=depth`) and the site column to be plotted on the x-axis (`x=site`).
- `+`: We need to have a `+` at the end of the `ggplot()` function to add the next component of the plot.
- `geom_boxplot()`: This adds a layer to our `ggplot2` object. In this case it converts the `ggplot` object, which is just information, into a boxplot.

Write and run the following code in a new cell:

```{r, eval=FALSE}
#Create ggplot2 boxplot of depth by size
ggplot2::ggplot(abundance_metadf, aes=(y=depth, x=site)) +
  ggplot2::geom_boxplot()
```

```{r, echo = FALSE}
opts_p <- c(answer="__LD (Lower Durance)__", "__MD (Middle Durance)__", "__UD (Upper Durance__")
```
- Which site has the highest median depth (middle line of boxplot)? `r longmcq(opts_p)`

We will use the same code to plot the depth by media.
You can copy and paste the code changing the x aesthetic to media (`x=media`).
Carry this out in the same cell and run the code.

```{r, eval=FALSE}
#Create ggplot2 boxplot of depth by size
ggplot2::ggplot(abundance_metadf, aes=(y=depth, x=media)) +
  ggplot2::geom_boxplot()
```

```{r, echo = FALSE}
opts_p <- c(answer="__CVP__", "__ENV__", "__TSA__")
```
- Which media has the lowest median depth (middle line of boxplot)? `r longmcq(opts_p)`

From the boxplots we can see there is no drastic difference between the depths of the different sample groups. We will therefore continue and make some rarefaction curves to further assess the depth of our samples.

For more resources on `ggplot2` please see the [appendix](#ggplot2_appendix) of this book.

## Rarefaction curve
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/rarefaction.png", auto_pdf = TRUE)
```

Our read depths appear a bit low, each sample has <20K reads. 
However, this might be fine for our dataset since we are using surface water samples rather than human gut microbiome samples.
Let's see how our samples look with a rarefaction curve.

__Note__: This is a quick example and we will go into more detail in the [rarefaction chapter](#rarefaction_chap).

Surprisingly, there is not a good method to produce a rarefaction curve with the `phyloseq` or `microbiome` packages. 
We will therefore use the [`vegan`](https://vegandevs.github.io/vegan/) package. 
`vegan` is an R package for community ecologists. 
It has a variety of functions but it uses normal R data frames rather than `phyloseq` objects.
We will therefore only use it for rarefaction purposes.

### ASV abundance data frame
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/nice_table.png", auto_pdf = TRUE)
```

Before creating our rarefaction curve we will extract the ASV abundance table from the `phyloseq` object with `phyloseq`'s function `otu_table()`. 
We need to transpose (`t()`) the table so it is in the correct orientation for the rarefaction function.
Additionally, we ensure it is a data frame with the function `as.data.frame()`.

Carry this out in a new cell.

```{r, eval=FALSE}
#Rarefaction curve
#Extract ASV table as data frame
asv_abund_df <- as.data.frame(t(phyloseq::otu_table(pseq)))
```

### vegan's rarecurve
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/vegan.png", auto_pdf = TRUE)
```

With this data frame we can create a rarefaction curve with `vegan`'s `rarecurve()` function.

Add the following code to the same cell and run it.

```{r, eval=FALSE}
#Rarefaction curve
vegan::rarecurve(
  x = asv_abund_df, step = 50,
  xlab = "Read depth",
  ylab = "ASVs"
)
```

In essence, we are hoping that the majority of samples have plateau'd.
If the curves have flattened in relation to the y axis this indicates that most of the ASVs present in the sample have been captured.

In this case the samples have plateau'd or have gentle slopes towards the end.
With this we can be happy to continue and not remove samples by a minimum read depth.

If we saw some samples with steep curves which had low depths we could carry out some more analysis with rarefaction.
However, more rarefaction functions, analysis, and theories will be covered in the [rarefaction chapter](#rarefaction_chap).
You could use some of these at this point to help you determine your minimum read depth for your own datasets.

## Filtering by minimum read depth
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/water_filter.png", auto_pdf = TRUE)
```

What if you want to filter samples by a minimum read depth?

In that case you can use the `subset_samples()` function from `phyloseq()`.
We will use our previously created vector containing read depths (`sample_depths`) to remove sample with less than 11k reads.
We have chosen this depth as an example to remove some samples.

Write and run the below code in a new cell.
It will create a new subsetted `phyloseq` object.

```{r, eval=FALSE}
#Subset and keep samples with at least 11k reads
pseq_min11K <- phyloseq::subset_samples(pseq, reads_sample > 11000)
```

After removing samples it is also useful to remove ASVs with no abundance values.
This can occur when ASVs are only present in the samples which have been removed.

To remove these ASVs we can use two `phyloseq` functions:

- __`taxa_sums()`__: Returns a vector showing sum of all taxa in the abundance table. 
  - In our `phyloseq` object the ASVs are the taxa. 
  - ASVs have long human unfriendly names that are unique to every single ASV possible.
- __`prune_taxa()`__: This retains taxa/ASVs based on a provided vector.
  - In this case we are creating a logical vector (`TRUE`/`FALSE`) where ASVs with 0 abundance are `FALSE` and ASVs with abundance > 0 are `TRUE`.

We'll first write and run some commands with `taxa_sums()` to get some practice with it. Carry this out int he same cell as the `subset_samples()` command.

```{r, eval=FALSE}
#Abundance sums of the 1st six ASVs
head(phyloseq::taxa_sums(pseq_min11K))
#View number of ASVs in our data
length(phyloseq::taxa_sums(pseq_min11K))
```

In the same cell add the following.
This will filter out ASVs with no abundance.

```{r, eval=FALSE}
#Remove ASVs with no abundance
pseq_min11k <- phyloseq::prune_taxa(
  phyloseq_taxa_sums(pseq_min11k) > 0, pseq_min11k
)
```

Finally, summarise the contents of the `phyloseq` object. 
Add the following in the same cell and then run the code in the cell.

```{r, eval=FALSE}
#Summarise subsetted phyloseq
microbiome::summarize_phyloseq(pseq_min11K)
microbiome::readcount(pseq_min11K)
pseq_min11k
```

We can see that the data lost 26 ASVs (2551 - 2525).

Try to write your own R code in a new cell to answer the following questions:

```{r, echo = FALSE}
opts_p <- c("__4__", answer="__613__", "__42507__")
```
- What is the difference of the minimum number of reads between `pseq_min11k` and `pseq`? r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__4__", "__613__", "__42507__")
```
- How many samples were removed due to the read depth filtering? r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__4__", "__613__", answer="__42507__")
```
- What is the difference of the minimum number of reads between `pseq_min11k` and `pseq`? r longmcq(opts_p)`

`r hide("Tip")`
Combining `microbiome::readcount()` with `min()`, `length()`, and `sum()` might help.
`r unhide()`

`r hide("Code solutions + bonus")`
```{r, eval=FALSE}
#Difference of minimum read numbers
min(microbiome::readcount(pseq_min11k)) - min(microbiome::readcount(pseq))
#Number of samples lost
length(microbiome::readcount(pseq)) - length(microbiome::readcount(pseq_min11k))
#Number of reads removed
sum(microbiome::readcount(pseq)) - sum(microbiome::readcount(pseq_min11k))
#Bonus
#List the removed samples
setdiff(phyloseq::sample_names(pseq)), phyloseq::sample_names(pseq_min11k))
```
`r unhide()`

We won't actually use this subsetted file as we want to keep all the samples in this case.
Additionally, since we didn't need remove any samples we don't need to remove any ASVs as they should all have a total abundance > 0.
Therefore you can remove the `phyloseq` object in a new cell.

```{r, eval=FALSE}
#Remove subsetted phyloseq
rm(pseq_min11K)
```

Once you are finished with this notebook you can save it then close and halt it.

## Minmum read depth: Summary
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/sum_blue.png", auto_pdf = TRUE)
```

We have assessed the read depth in this chapter and decided to not remove any samples. 
This assessment included:

- Viewing a histogram of sample read depths.
- Creating boxplots to compare the sample read depths across sample groups (siet and media).
- Producing a rarefaction depth to determine if any samples did not represent a good amount of the ASVs present in the environment.

Additionally, we created a new `phyloseq` object where the samples were filtered by depth.
Ultimately we did not keep the filtered `phyloseq` object.

<!--chapter:end:09-Minimum_read_depth.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Relative abundance

In this chapter we are going to create a `phyloseq` with relative abundance values. We would use our read depth filtered data if we wanted to use it for other downstream analyses.
But, as stated in the last chapter all our samples looked good so we will use our original abundance `phyloseq` object.

Once we have created our relative abundance data we will remove rare ASVs.
We will then check how many ASVs we lost and save the relative abundance `phyloseq` object.

## Relative abundance: setup

The first steps before analysis are:

- Create a new notebook called __"3-relative_abundance.ipynb"__.
- Add a markdown cell with the first level header: `# Relative abundance`.
- Add the below to a code cell to load in the `phyloseq` object and libraries.

```{r, eval=FALSE}
#Libraries
library("phyloseq")
library("microbiome")
#Load phyloseq object
load("phyloseq.RData")
```

From now on you will get less instructions on your notebook structure. 
Please create your own coding and markdown cells where you think appropriate.

## Relative abundance: transformation

Now that we have the data loaded we can create a new `phyloseq` object by converting the abundance values to relative abundance.

This is carried out with the `microbiome` function `transform()`. With it we transform the ASV abundance table within to a "compositional" table (relative abundance). 

Run the below command in an appropriate place in your notebook:

```{r, eval=FALSE}
#Convert abundance table to relative abundance (compositional) table
pseq_relabund <- microbiome::transform(pseq, "compositional")
```

As always it is good to check the contents of the new ASV table.

```{r, eval=FALSE}
#Summarise and check sample counts which should each amount to 1
microbiome::summarize_phyloseq(pseq_relabund)
microbiome::readcount(pseq_relabund)
```

You will notice the read count for each sample is __1__. This abundance table contains fractional relative abundances for each ASV. This fraction is relative to the total abundance within each sample. Therefore, all the fractional relative abundance of ASVs in one sample total 1.

## Rare ASV removal

When we are using total or rarefied abundance values we want to keep rare ASVs, singletons, and doubletons. These are important to keep so we can get detailed data. Additionally, some alpha diversity metrics require these values.

`r hide("Singletons & Doubletons defintion")`

- __Singleton__: A single, unique DNA sequence. I.e. an ASV with a total abundance of 1 across all samples.
- __Doubleton__: A DNA sequence that only appears twice. I.e. an ASV with a total abundance of 2 across all samples.

`r unhide()`

However, when we are using relative abundance data it can be a good idea to remove rare features (ASVs). Relative abundance data is good to get a broader picture and therefore rare ASVs may add too much noise.

We will therefore remove ASVs with low relative abundances. A recommended method is to remove ASVs with a mean relative abundance equal to or less than `1e-5` (0.00001).

For this we will use `phyloseq::filter_taxa()`. This uses a function to keep ASVs with a mean relative abundance > 1e-5.

```{r, eval=FALSE}
#Remove rare ASVs, retain ASVs with mean > 1e-5
pseq_relabund <- phyloseq::filter_taxa(
  pseq_relabund, function(x) mean(x) > 1e-5, TRUE
)
```

It is very important to make sure only a small amount of our relative abundance has been lost across our samples. Check this with the following commands.

```{r, eval=FALSE}
#Summarise and check sample counts which should each amount to around 1
microbiome::summarize_phyloseq(pseq_relabund)
microbiome::readcount(pseq_relabund)
```

```{r, echo = FALSE}
opts_p <- c(answer="__0.979__", "__0.997__", "__1__")
```
  - What is the approximate minimum total relative abundance? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__0.978__", answer="__0.997__", "__1__")
```
  - What is the approximate average total relative abundance? `r longmcq(opts_p)`

## Relative abundance: check




#Summarise and check sample counts which should each amount to 1


#Check the below logic
#When using total abundance values it is useful to have 0 values, singletons, and doubletons
#This is because some alpha diversity metrics require them
#However it is useful to remove low relative abundance data in relative abundance data
#This is so the rare ASVs do not overly affect certain types of analysis

#first remove ASV with relabund equal to 0
#This can occur if samples were removed which had ASVs 
#not present in the remaining samples
pseq_relabund <-  filter_taxa(pseq_relabund, function(x) sum(x) > 0, TRUE)

#Summarise and check sample counts which should each amount to around 1
microbiome::summarize_phyloseq(pseq_relabund)
microbiome::readcount(pseq_relabund)

#All the total relative abundances still equal 1
#This is expected since no samples were removed
#As this is the case there is no need to check if ASVs were removed

#We will now remove rare ASVs as these are not as useful in relative abundance data
#compared to abundance data
#There are many ways to do this
#A common way, recommended by the phyloseq developer
#Remove ASVs with a mean (across samples) less than 1e-5 (relabund)
pseq_relabund <-  
  phyloseq::filter_taxa(
    pseq_relabund, function(x) mean(x) > 1e-5, TRUE)

#Summarise and check sample counts which should each amount to around 1
microbiome::summarize_phyloseq(pseq_relabund)
microbiome::readcount(pseq_relabund)

#Total relative abundance has decreased by a very small amount
#This is what we are looking for, if too much is being removed >0.05
#you will need to try to be gentler with the filtering
#Such as trying 1e-6 rather than 1e-5

#We should also check how many ASVs have been removed
num_asvs_vec["relabund"] <- nrow(phyloseq::otu_table(pseq_relabund))
num_asvs_vec
num_asvs_vec["abundance"] - num_asvs_vec["relabund"]

#We have lost a good amount of ASVs but these only equate to a very small
#amount relabund. This is fine as we generally use relative abundance
#when looking at the larger picture rather than at closer pictures
#instead we can use a rarefied abundance table to look at the closer picture

#We are now happy with our relative abundance table
#Therefore we can save it for further use
save(pseq_relabund, file = "phyloseq_relabund.RData")

<!--chapter:end:10-Relative_abundance.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Rarefaction {#rarefaction_chap}

#Rarefy abundance table ####
#i.e. convert abundance numbers so each sample has equal depth

#Before rarefying a table it is good to make a rarefaction curve
#This is to help us choose an appropriate rarefaction threshold

#We will use the very useful package vegan
#Ignore any warning message
vegan::rarecurve(
  x = as.data.frame(t(phyloseq::otu_table(pseq))), step = 50)

#Let us improve this and save it into a file
png(filename = "./rarefaction_plot.png", res = 300,
    units = "mm", height = 200, width = 300)
vegan::rarecurve(
  x = as.data.frame(t(phyloseq::otu_table(pseq))), step = 50,
  ylab="ASVs", lwd=1,label=F)
#Add a vertical line of the smallest sample depth
abline(v = min(reads_sample), col="red")
dev.off()

#With this we can see that the majorty of samples plateau at 
#the minimum sampleing depth
#Therefore we can use this as a rarefaction size
pseq_rarefy <- 
  phyloseq::rarefy_even_depth(
    pseq, sample.size = min(reads_sample), rngseed = 1000)

#Summarise and check sample counts which should each amount to 10433
microbiome::summarize_phyloseq(pseq_rarefy)
microbiome::readcount(pseq_rarefy)

#Check ASVs
num_asvs_vec["rarefied"] <- nrow(phyloseq::otu_table(pseq_rarefy))
num_asvs_vec

#Save phyloseq object
save(pseq_relpseq_rarefyabund, file = "phyloseq_rarefied.RData")

<!--chapter:end:11-Rarefaction.Rmd-->

